ğŸ“Š Analyse des SÃ©ries Temporelles - Projet Complet
ğŸ¯ Description du Projet
Ce projet prÃ©sente une analyse complÃ¨te des techniques de prÃ©vision des sÃ©ries temporelles, allant des mÃ©thodes statistiques traditionnelles aux modÃ¨les avancÃ©s de Deep Learning (RNN, LSTM, CNN, Transformers). L'objectif est de fournir une boÃ®te Ã  outils complÃ¨te pour la prÃ©diction de sÃ©ries temporelles avec des exemples pratiques et des benchmarks comparatifs.

ğŸ“ Structure du Projet
text
time-series-project/
â”œâ”€â”€ README.md
â”œâ”€â”€ time_series_analysis.ipynb
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ generated_series.npy
â”‚   â””â”€â”€ time_series.csv
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ naive_model.pkl
â”‚   â”œâ”€â”€ nn_model.h5
â”‚   â”œâ”€â”€ lstm_model.h5
â”‚   â””â”€â”€ transformer_model.h5
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_preprocessing.py
â”‚   â”œâ”€â”€ models.py
â”‚   â””â”€â”€ utils.py
â””â”€â”€ results/
    â”œâ”€â”€ predictions/
    â””â”€â”€ visualizations/
ğŸš€ Installation
PrÃ©requis
Python 3.8+

pip ou conda

Installation des dÃ©pendances
bash
pip install -r requirements.txt
DÃ©pendances principales
txt
numpy==1.24.3
pandas==2.0.3
matplotlib==3.7.2
tensorflow==2.13.0
statsmodels==0.14.0
scikit-learn==1.3.0
jupyter==1.0.0
ğŸ“Š DonnÃ©es
GÃ©nÃ©ration de donnÃ©es synthÃ©tiques
Le projet utilise des donnÃ©es synthÃ©tiques avec tendance, saisonnalitÃ© et bruit :

python
import numpy as np
import matplotlib.pyplot as plt

# ParamÃ¨tres
time = np.arange(4 * 365 + 1)
baseline = 10
amplitude = 40
slope = 0.05
noise_level = 5

# GÃ©nÃ©ration de la sÃ©rie
series = baseline + slope * time
series += amplitude * np.sin(time / 365 * 2 * np.pi)
series += np.random.normal(scale=noise_level, size=len(time))
Division des donnÃ©es
DonnÃ©es d'entraÃ®nement : 1000 premiers points

DonnÃ©es de validation : points restants

ğŸ§  ModÃ¨les ImplÃ©mentÃ©s
1. MÃ©thodes Statistiques
PrÃ©vision naÃ¯ve : DerniÃ¨re valeur observÃ©e

Moyenne mobile : FenÃªtres glissantes

DiffÃ©renciation : Ã‰limination tendance/saisonnalitÃ©

2. RÃ©seaux de Neurones
NN simple : 1 couche dense (MSE: 46.99)

Deep NN : 2 couches 10â†’10 (MSE: ~25-30)

RNN/LSTM/GRU : ModÃ¨les rÃ©currents

3. Architectures AvancÃ©es
CNN 1D : Convolutions temporelles

Transformer : Attention multi-tÃªtes

ğŸ“ˆ RÃ©sultats
Comparaison des performances
ModÃ¨le	MSE	MAE	Temps d'entraÃ®nement
PrÃ©vision naÃ¯ve	50.63	5.61	< 1s
Moyenne mobile	31.45	4.44	< 1s
NN simple	46.99	4.97	~2 min
Deep NN	25-30	4-5	~5 min
LSTM	Variable	Variable	~10 min
CNN	357.95	14.89	~8 min
ğŸ”§ Utilisation
1. ExÃ©cution du notebook
bash
jupyter notebook time_series_analysis.ipynb
2. EntraÃ®nement d'un modÃ¨le
python
from src.models import TimeSeriesModel

# Initialisation du modÃ¨le
model = TimeSeriesModel(window_size=30)

# EntraÃ®nement
model.train(x_train, epochs=100, validation_data=x_valid)

# PrÃ©diction
predictions = model.predict(x_valid)
3. Visualisation des rÃ©sultats
python
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.plot(x_valid, label='Valeurs rÃ©elles')
plt.plot(predictions, label='PrÃ©dictions', alpha=0.7)
plt.legend()
plt.title('Comparaison prÃ©dictions vs rÃ©alitÃ©')
plt.show()
ğŸ¯ FonctionnalitÃ©s ClÃ©s
âœ… PrÃ©-traitement intelligent
DiffÃ©renciation pour stationnaritÃ©

FenÃªtrage glissant configurable

Normalisation automatique

âœ… ModÃ¨les variÃ©s
7 architectures diffÃ©rentes

HyperparamÃ¨tres optimisables

Sauvegarde/chargement des modÃ¨les

âœ… Ã‰valuation complÃ¨te
MÃ©triques MSE et MAE

Visualisations interactives

Comparaison des performances

âœ… Production ready
Code modulaire

Documentation complÃ¨te

Facile Ã  Ã©tendre

ğŸ“ Exemple de Code
CrÃ©ation d'un modÃ¨le LSTM
python
import tensorflow as tf
from tensorflow.keras import layers

def create_lstm_model(window_size):
    model = tf.keras.Sequential([
        layers.Input(shape=(window_size, 1)),
        layers.LSTM(64, return_sequences=True),
        layers.LSTM(32),
        layers.Dense(16, activation='relu'),
        layers.Dense(1)
    ])
    
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        loss='mse',
        metrics=['mae']
    )
    
    return model
Pipeline d'entraÃ®nement complet
python
# 1. Chargement des donnÃ©es
data = load_time_series('data/time_series.csv')

# 2. PrÃ©-traitement
processed_data = preprocess_data(data, window_size=30)

# 3. Division train/validation
x_train, x_valid = split_data(processed_data, split_time=1000)

# 4. CrÃ©ation du modÃ¨le
model = create_lstm_model(window_size=30)

# 5. EntraÃ®nement
history = model.fit(
    x_train,
    epochs=100,
    validation_data=x_valid,
    callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)]
)

# 6. Ã‰valuation
mse, mae = model.evaluate(x_valid)
print(f"MSE: {mse:.2f}, MAE: {mae:.2f}")
ğŸ“Š Visualisations Disponibles
SÃ©rie temporelle originale

DÃ©composition (tendance, saisonnalitÃ©, rÃ©sidus)

AutocorrÃ©lation

PrÃ©dictions vs rÃ©alitÃ©

Courbes d'apprentissage

Comparaison des modÃ¨les

ğŸš€ DÃ©ploiement
ExÃ©cution rapide
bash
# Clonez le dÃ©pÃ´t
git clone https://github.com/votre-utilisateur/time-series-project.git

# Installez les dÃ©pendances
cd time-series-project
pip install -r requirements.txt

# ExÃ©cutez le notebook
jupyter notebook time_series_analysis.ipynb
Docker (optionnel)
dockerfile
FROM python:3.9
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["jupyter", "notebook", "--ip=0.0.0.0", "--port=8888", "--no-browser", "--allow-root"]
ğŸ¤ Contribution
Les contributions sont les bienvenues ! Voici comment contribuer :

Fork le projet

CrÃ©ez une branche (git checkout -b feature/AmazingFeature)

Committez vos changements (git commit -m 'Add some AmazingFeature')

Push vers la branche (git push origin feature/AmazingFeature)

Ouvrez une Pull Request

ğŸ“š Documentation SupplÃ©mentaire
Concepts thÃ©oriques couverts :
StationnaritÃ© et tests ADF

AutocorrÃ©lation et autocorrÃ©lation partielle

DiffÃ©renciation et dÃ©saisonnalisation

Validation croisÃ©e temporelle

MÃ©triques d'Ã©valuation spÃ©cifiques aux sÃ©ries temporelles

Techniques avancÃ©es :
Learning rate scheduling

Early stopping dynamique

Regularisation pour sÃ©ries temporelles

Ensembling de modÃ¨les

Features engineering temporel

